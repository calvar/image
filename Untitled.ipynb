{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "a3ba87e7",
   "metadata": {},
   "source": [
    "https://pytorch.org/tutorials/beginner/introyt/trainingyt.html <br>\n",
    "Instalar: torch, torchvision, tensorboard, matplotlib"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "6c27b783",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from datetime import datetime\n",
    "\n",
    "import torch\n",
    "import torchvision\n",
    "from torchvision.transforms import Compose, Normalize, ToTensor\n",
    "from torchvision.datasets import FashionMNIST\n",
    "from torch.utils.data import DataLoader\n",
    "from torch.utils.tensorboard import SummaryWriter\n",
    "\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "d9de4485",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Define a transform object that takes the data to pytorch tensor form and normalizes it\n",
    "transform = Compose( [ToTensor(), Normalize((0.5,),(0.5,))] );"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "72d8135a",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Get the fashion mnist set of images, and create the train and test data sets\n",
    "training_set = FashionMNIST('.data', train=True, transform=transform, download=True);\n",
    "validation_set = FashionMNIST('.data', train=False, transform=transform, download=True);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "3ce26af5",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Prepare batches that update at every epoch for training. Use multiprocessing\n",
    "training_loader = DataLoader(training_set, batch_size=4, shuffle=True, num_workers=2);\n",
    "validation_loader = DataLoader(validation_set, batch_size=4, shuffle=False, num_workers=2);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "5262c667",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Define class labels\n",
    "classes = ('T-shirt/top', 'Trouser', 'Pullover', 'Dress', 'Coat', 'Sandal', 'Shirt', 'Sneaker', 'Bag', 'Ankle Boot');"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "5c6ef745",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training set has 60000 instances\n",
      "Validation set has 10000 instances\n"
     ]
    }
   ],
   "source": [
    "#Report split sizes\n",
    "print('Training set has {} instances'.format(len(training_set)));\n",
    "print('Validation set has {} instances'.format(len(validation_set)));"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "68c49fb6",
   "metadata": {},
   "source": [
    "## Visualization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "7dbeb96d",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Helper function for inline image display\n",
    "def matplotlib_imshow(img, one_channel=False):\n",
    "    if one_channel:\n",
    "        img = img.mean(dim=0)\n",
    "    img = img / 2 + 0.5\n",
    "    npimg = img.numpy()\n",
    "    if one_channel:\n",
    "        plt.imshow(npimg, cmap=\"Greys\")\n",
    "    else:\n",
    "        plt.imshow(np.transpose(npimg, (1,2,0)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "5a3a0e22",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Create an iterator to go through the training data\n",
    "dataiter = iter(training_loader);\n",
    "#Take the next minibatch of data\n",
    "images, labels = dataiter.next(); "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "a8c6d30d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sandal Ankle Boot Sandal Shirt\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "#Create a grid from the images and show them\n",
    "img_grid = torchvision.utils.make_grid(images);\n",
    "matplotlib_imshow(img_grid, one_channel=True);\n",
    "print(' '.join(classes[labels[j]] for j in range(4)));"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2cdd786c",
   "metadata": {},
   "source": [
    "## The model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "8c8a12b8",
   "metadata": {},
   "outputs": [],
   "source": [
    "#pytorch models inherit from torch.nn.Module\n",
    "class GarmentClassifier(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(GarmentClassifier, self).__init__()\n",
    "        self.conv1 = nn.Conv2d(1, 6, 5) #Add a convolution layer thet resives a single layer as imput (grayscale), uses 6 filters on it, and each filter has 5x5 size\n",
    "        self.pool = nn.MaxPool2d(2,2) #Pooling layer. Operation to get the max value over 2x2 (stride 2) sections the filter and reduce the number of features\n",
    "        self.conv2 = nn.Conv2d(6, 16, 5) #Second convolution layer (6 chanel imput, from the 6 filters of previous layer, 16 filters, and 5x5 filter size)\n",
    "        self.fc1 = nn.Linear(16*4*4, 120) #1st linear layer to classify the compressed features (why is imput 16*4*4?)\n",
    "        self.fc2 = nn.Linear(120, 84) #2nd linear layer\n",
    "        self.fc3 = nn.Linear(84, 10) #3rd linear layer. output is size 10 because there are 10 classes\n",
    "        \n",
    "    # Forward passing of the data\n",
    "    def forward(self, x):\n",
    "        x = self.pool(F.relu(self.conv1(x))) # pass x input to conv layer 1 and apply relu to the result, then do pooling to reduce number of features?\n",
    "        x = self.pool(F.relu(self.conv2(x))) # then do the same using conv layer 2?\n",
    "        x = x.view(-1, 16*4*4) # flatten x from 2d to 1d??\n",
    "        x = F.relu(self.fc1(x)) #Apply relu to output of linear layer 1\n",
    "        x = F.relu(self.fc2(x)) #Apply relu to output of linear layer 2\n",
    "        x = self.fc3(x) #Pass output through layer 3 and get final result\n",
    "        return x\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "fcbca866",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = GarmentClassifier()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fd9f29b0",
   "metadata": {},
   "source": [
    "## Loss function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "d70428dc",
   "metadata": {},
   "outputs": [],
   "source": [
    "## use the log loss as cost function\n",
    "loss_fn = nn.CrossEntropyLoss()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "8506f710",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[6.6849e-01, 2.1233e-01, 2.7045e-02, 9.8154e-01, 5.5817e-01, 8.0049e-01,\n",
      "         3.4999e-01, 1.8019e-01, 8.0567e-01, 1.6934e-04],\n",
      "        [4.4766e-02, 8.5614e-01, 5.9486e-01, 6.4487e-01, 1.1214e-01, 3.9759e-01,\n",
      "         7.2651e-01, 1.0874e-01, 4.2925e-01, 6.9066e-01],\n",
      "        [6.5272e-01, 1.6306e-01, 1.7622e-01, 5.2290e-01, 5.0913e-02, 7.1940e-02,\n",
      "         6.6595e-01, 9.8815e-01, 1.3647e-01, 2.0910e-02],\n",
      "        [5.6513e-01, 1.1180e-01, 5.5177e-01, 9.3320e-01, 3.1780e-01, 2.2516e-01,\n",
      "         8.8686e-02, 3.7950e-02, 2.6389e-01, 7.7027e-01]])\n",
      "tensor([1, 5, 3, 7])\n",
      "Total loss for this batxh: 2.4699039459228516\n"
     ]
    }
   ],
   "source": [
    "##Test the loss function##\n",
    "#Pass data in batches of 4\n",
    "#Returns 10 lists of 4 random numbers between 0 and 1 as a 4x10 tensor, to test confidence on each of the 10 classes\n",
    "dummy_outputs = torch.rand(4, 10)\n",
    "# Represents the correct class among the possible 10\n",
    "dummy_labels = torch.tensor([1, 5, 3, 7])\n",
    "\n",
    "print(dummy_outputs)\n",
    "print(dummy_labels)\n",
    "\n",
    "loss = loss_fn(dummy_outputs, dummy_labels)\n",
    "print('Total loss for this batxh: {}'.format(loss.item()))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "855b5e0a",
   "metadata": {},
   "source": [
    "## Optimizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "791a1ebe",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Stochastic gradient descent optimizer\n",
    "optimizer = torch.optim.SGD(model.parameters(), lr=0.001, momentum=0.9)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2c792a7b",
   "metadata": {},
   "source": [
    "## Training loop\n",
    "- Get a batch of data from the Data Loader\n",
    "- Zero the optimizer's gradients\n",
    "- Gets predictions from the model for an imput batch\n",
    "- Computes the loss function for the prediction\n",
    "- Adjusts the model's weights with gradient descent\n",
    "- Reports the loss for every 1000 batches\n",
    "- Reports the average per-batch loss for the last 1000 batches for comparison to a validation run"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "fb14913d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_one_epoch(epoch_index, tb_writer):\n",
    "    running_loss = 0\n",
    "    last_loss = 0\n",
    "    \n",
    "    #Use an enumeration istead of an iterator to track the batch index and do reporting\n",
    "    for i, data in enumerate(training_loader):\n",
    "        #training instances are input + label pairs\n",
    "        inputs, labels = data\n",
    "        \n",
    "        #zero the gradients\n",
    "        optimizer.zero_grad()\n",
    "        \n",
    "        #make predictions fo this batch\n",
    "        outputs = model(inputs)\n",
    "        \n",
    "        #compute loss and its gradient\n",
    "        loss = loss_fn(outputs, labels)\n",
    "        loss.backward()\n",
    "        \n",
    "        #adjust the weights\n",
    "        optimizer.step()\n",
    "        \n",
    "        #gather data and report\n",
    "        running_loss += loss.item()\n",
    "        if i % 1000 == 999:\n",
    "            last_loss = running_loss / 1000 #loss per batch\n",
    "            print(' batch {} loss: {}'.format(i+1, last_loss))\n",
    "            tb_x = epoch_index * len(training_loader) + i + 1\n",
    "            tb_writer.add_scalar('Loss/train', last_loss, tb_x)\n",
    "            running_loss = 0\n",
    "            \n",
    "    return last_loss"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4e50fda8",
   "metadata": {},
   "source": [
    "## Main loop\n",
    "Before training an epoch there are a couple of things to do:\n",
    "- Validate by checking the relative loss on a set of validation data and report it\n",
    "- Save a copy of the model\n",
    "\n",
    "We use TensorBoard to do the reporting."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "5f6d2f50",
   "metadata": {},
   "outputs": [],
   "source": [
    "timestamp = datetime.now().strftime('%Y%m%d_%H%M%S')\n",
    "writer = SummaryWriter('runs/fashion_trainer_{}'.format(timestamp))\n",
    "epoch_number = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "6480a378",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "EPOCH 1\n",
      " batch 1000 loss: 0.34065542298334184\n",
      " batch 2000 loss: 0.31489107795442395\n",
      " batch 3000 loss: 0.324787577760726\n",
      " batch 4000 loss: 0.3094142994025988\n",
      " batch 5000 loss: 0.3230147731143225\n",
      " batch 6000 loss: 0.32943408398489554\n",
      " batch 7000 loss: 0.33273859233935943\n",
      " batch 8000 loss: 0.33398749118854176\n",
      " batch 9000 loss: 0.3129590532753937\n",
      " batch 10000 loss: 0.33826820104951183\n",
      " batch 11000 loss: 0.3103328282620569\n",
      " batch 12000 loss: 0.32363992346724263\n",
      " batch 13000 loss: 0.31736081921453296\n",
      " batch 14000 loss: 0.3264538077777834\n",
      " batch 15000 loss: 0.3079342334113753\n"
     ]
    },
    {
     "ename": "RuntimeError",
     "evalue": "Too many open files. Communication with the workers is no longer possible. Please increase the limit using `ulimit -n` in the shell or change the sharing strategy by calling `torch.multiprocessing.set_sharing_strategy('file_system')` at the beginning of your code",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "Cell \u001b[0;32mIn [22], line 16\u001b[0m\n\u001b[1;32m     14\u001b[0m \u001b[38;5;66;03m#Pull batches fron validation data to validate\u001b[39;00m\n\u001b[1;32m     15\u001b[0m running_vloss \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m0\u001b[39m\n\u001b[0;32m---> 16\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m i, vdata \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28menumerate\u001b[39m(validation_loader):\n\u001b[1;32m     17\u001b[0m     vinputs, vlabels \u001b[38;5;241m=\u001b[39m vdata\n\u001b[1;32m     18\u001b[0m     voutputs \u001b[38;5;241m=\u001b[39m model(vinputs)\n",
      "File \u001b[0;32m~/Documents/MachineLearning/images/venv/lib/python3.8/site-packages/torch/utils/data/dataloader.py:681\u001b[0m, in \u001b[0;36m_BaseDataLoaderIter.__next__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    678\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_sampler_iter \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m    679\u001b[0m     \u001b[38;5;66;03m# TODO(https://github.com/pytorch/pytorch/issues/76750)\u001b[39;00m\n\u001b[1;32m    680\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_reset()  \u001b[38;5;66;03m# type: ignore[call-arg]\u001b[39;00m\n\u001b[0;32m--> 681\u001b[0m data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_next_data\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    682\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_num_yielded \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1\u001b[39m\n\u001b[1;32m    683\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_dataset_kind \u001b[38;5;241m==\u001b[39m _DatasetKind\u001b[38;5;241m.\u001b[39mIterable \u001b[38;5;129;01mand\u001b[39;00m \\\n\u001b[1;32m    684\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_IterableDataset_len_called \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mand\u001b[39;00m \\\n\u001b[1;32m    685\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_num_yielded \u001b[38;5;241m>\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_IterableDataset_len_called:\n",
      "File \u001b[0;32m~/Documents/MachineLearning/images/venv/lib/python3.8/site-packages/torch/utils/data/dataloader.py:1359\u001b[0m, in \u001b[0;36m_MultiProcessingDataLoaderIter._next_data\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   1356\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_process_data(data)\n\u001b[1;32m   1358\u001b[0m \u001b[38;5;28;01massert\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_shutdown \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_tasks_outstanding \u001b[38;5;241m>\u001b[39m \u001b[38;5;241m0\u001b[39m\n\u001b[0;32m-> 1359\u001b[0m idx, data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_get_data\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1360\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_tasks_outstanding \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1\u001b[39m\n\u001b[1;32m   1361\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_dataset_kind \u001b[38;5;241m==\u001b[39m _DatasetKind\u001b[38;5;241m.\u001b[39mIterable:\n\u001b[1;32m   1362\u001b[0m     \u001b[38;5;66;03m# Check for _IterableDatasetStopIteration\u001b[39;00m\n",
      "File \u001b[0;32m~/Documents/MachineLearning/images/venv/lib/python3.8/site-packages/torch/utils/data/dataloader.py:1325\u001b[0m, in \u001b[0;36m_MultiProcessingDataLoaderIter._get_data\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   1321\u001b[0m     \u001b[38;5;66;03m# In this case, `self._data_queue` is a `queue.Queue`,. But we don't\u001b[39;00m\n\u001b[1;32m   1322\u001b[0m     \u001b[38;5;66;03m# need to call `.task_done()` because we don't use `.join()`.\u001b[39;00m\n\u001b[1;32m   1323\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m   1324\u001b[0m     \u001b[38;5;28;01mwhile\u001b[39;00m \u001b[38;5;28;01mTrue\u001b[39;00m:\n\u001b[0;32m-> 1325\u001b[0m         success, data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_try_get_data\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1326\u001b[0m         \u001b[38;5;28;01mif\u001b[39;00m success:\n\u001b[1;32m   1327\u001b[0m             \u001b[38;5;28;01mreturn\u001b[39;00m data\n",
      "File \u001b[0;32m~/Documents/MachineLearning/images/venv/lib/python3.8/site-packages/torch/utils/data/dataloader.py:1190\u001b[0m, in \u001b[0;36m_MultiProcessingDataLoaderIter._try_get_data\u001b[0;34m(self, timeout)\u001b[0m\n\u001b[1;32m   1188\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mOSError\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[1;32m   1189\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m e\u001b[38;5;241m.\u001b[39merrno \u001b[38;5;241m==\u001b[39m errno\u001b[38;5;241m.\u001b[39mEMFILE:\n\u001b[0;32m-> 1190\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mRuntimeError\u001b[39;00m(\n\u001b[1;32m   1191\u001b[0m             \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mToo many open files. Communication with the\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m   1192\u001b[0m             \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m workers is no longer possible. Please increase the\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m   1193\u001b[0m             \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m limit using `ulimit -n` in the shell or change the\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m   1194\u001b[0m             \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m sharing strategy by calling\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m   1195\u001b[0m             \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m `torch.multiprocessing.set_sharing_strategy(\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mfile_system\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m)`\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m   1196\u001b[0m             \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m at the beginning of your code\u001b[39m\u001b[38;5;124m\"\u001b[39m) \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;28mNone\u001b[39m\n\u001b[1;32m   1197\u001b[0m \u001b[38;5;28;01mraise\u001b[39;00m\n",
      "\u001b[0;31mRuntimeError\u001b[0m: Too many open files. Communication with the workers is no longer possible. Please increase the limit using `ulimit -n` in the shell or change the sharing strategy by calling `torch.multiprocessing.set_sharing_strategy('file_system')` at the beginning of your code"
     ]
    }
   ],
   "source": [
    "EPOCHS = 5\n",
    "best_vloss = 1_000_000.\n",
    "\n",
    "for epoch in range(EPOCHS):\n",
    "    print('EPOCH {}'.format(epoch_number + 1))\n",
    "    \n",
    "    #Make sure gradient tracking is on, and do a pass over the data\n",
    "    model.train(True)\n",
    "    avg_loss = train_one_epoch(epoch_number, writer)\n",
    "    \n",
    "    #To do reporting gradients do not need to be on\n",
    "    model.train(False)\n",
    "    \n",
    "    #Pull batches fron validation data to validate\n",
    "    running_vloss = 0\n",
    "    for i, vdata in enumerate(validation_loader):\n",
    "        vinputs, vlabels = vdata\n",
    "        voutputs = model(vinputs)\n",
    "        vloss = loss_fn(voutputs, vlabels)\n",
    "        running_vloss += vloss\n",
    "    \n",
    "    avg_vloss = running_vloss / (i+1)\n",
    "    print('LOSS train {} valid {}'.format(avg_loss, avg_vloss))\n",
    "    \n",
    "    #Log the runnig loss averaged per batch for both training and validation\n",
    "    writer.add_scalars('Training vs. Validation Loss',\n",
    "                      {'Training': avg_loss, 'Validation': avg_vloss},\n",
    "                      epoch_number + 1)\n",
    "    writer.flush()\n",
    "    \n",
    "    # Track best performance, and save the model's state\n",
    "    if avg_vloss < best_vloss:\n",
    "        best_vloss = avg_vloss\n",
    "        model_path = 'model_{}_{}'.format(timestamp, epoch_number)\n",
    "        torch.save(model.state_dict(), model_path)\n",
    "        \n",
    "    epoch_number += 1\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
